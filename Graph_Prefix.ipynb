{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fangyihao/graphprefix/blob/main/Graph_Prefix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xodM0OE4DUHq",
        "outputId": "13fc89be-f661-407a-a471-f6fb8787e1f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'graphprefix'...\n",
            "remote: Enumerating objects: 186, done.\u001b[K\n",
            "remote: Counting objects: 100% (186/186), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 186 (delta 55), reused 148 (delta 35), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (186/186), 541.33 KiB | 3.34 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/fangyihao/graphprefix.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfRf6XXDb9bW",
        "outputId": "78971133-b54d-4046-f0fb-33c2c6849321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/graphprefix\n"
          ]
        }
      ],
      "source": [
        "cd /content/graphprefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc1G_AGAbqCI",
        "outputId": "51298b0c-be7d-4bb0-fb60-758c30cab52b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "arguments.py\t\t    prefix_fusion.py\t  run_pt_test.sh\n",
            "data\t\t\t    preprocess.py\t  run.py\n",
            "download_raw_data.sh\t    preprocess_utils\t  runs\n",
            "eval_prefix_fusion.sh\t    pt_test.py\t\t  run_script\n",
            "figures\t\t\t    __pycache__\t\t  search.py\n",
            "gat_test.py\t\t    README.md\t\t  search_script\n",
            "high_level_methodology.jpg  requirements.txt\t  tasks\n",
            "LICENSE\t\t\t    run_gat_test.sh\t  training\n",
            "model\t\t\t    run_prefix_fusion.sh  utils\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiOR7-g2cCUD"
      },
      "outputs": [],
      "source": [
        "!rm -r data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIF2sV8BEN8-",
        "outputId": "f90522f8-3c7a-42dd-aa84-acb2b6bc9279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12z67Nyiuo45HHgg-fPMM1mE55aW8-vi7\n",
            "To: /content/graphprefix/data.zip\n",
            "100% 4.22G/4.22G [00:57<00:00, 72.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 12z67Nyiuo45HHgg-fPMM1mE55aW8-vi7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZb2WKDEEiFT",
        "outputId": "d086d461-1da9-430f-d457-dbf9bb8b4610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: __MACOSX/._data         \n",
            "   creating: data/cpnet/\n",
            "  inflating: __MACOSX/data/._cpnet   \n",
            "   creating: data/obqa/\n",
            "  inflating: __MACOSX/data/._obqa    \n",
            "   creating: data/csqa/\n",
            "  inflating: __MACOSX/data/._csqa    \n",
            "  inflating: data/cpnet/conceptnet-assertions-5.6.0.csv  \n",
            "  inflating: __MACOSX/data/cpnet/._conceptnet-assertions-5.6.0.csv  \n",
            "  inflating: data/cpnet/conceptnet.en.unpruned.graph  \n",
            "  inflating: __MACOSX/data/cpnet/._conceptnet.en.unpruned.graph  \n",
            "  inflating: data/cpnet/conceptnet.en.pruned.graph  \n",
            "  inflating: __MACOSX/data/cpnet/._conceptnet.en.pruned.graph  \n",
            "  inflating: data/cpnet/tzw.ent.npy  \n",
            "  inflating: __MACOSX/data/cpnet/._tzw.ent.npy  \n",
            "  inflating: data/cpnet/matcher_patterns.json  \n",
            "  inflating: __MACOSX/data/cpnet/._matcher_patterns.json  \n",
            "  inflating: data/cpnet/conceptnet.en.csv  \n",
            "  inflating: __MACOSX/data/cpnet/._conceptnet.en.csv  \n",
            "  inflating: data/cpnet/concept.txt  \n",
            "  inflating: __MACOSX/data/cpnet/._concept.txt  \n",
            "   creating: data/obqa/graph/\n",
            "  inflating: __MACOSX/data/obqa/._graph  \n",
            "   creating: data/obqa/fairseq/\n",
            "  inflating: __MACOSX/data/obqa/._fairseq  \n",
            "   creating: data/obqa/statement/\n",
            "  inflating: __MACOSX/data/obqa/._statement  \n",
            "   creating: data/obqa/grounded/\n",
            "  inflating: __MACOSX/data/obqa/._grounded  \n",
            "   creating: data/obqa/OpenBookQA-V1-Sep2018/\n",
            "  inflating: __MACOSX/data/obqa/._OpenBookQA-V1-Sep2018  \n",
            "  inflating: data/csqa/train_rand_split.jsonl  \n",
            "  inflating: __MACOSX/data/csqa/._train_rand_split.jsonl  \n",
            "   creating: data/csqa/graph/\n",
            "  inflating: __MACOSX/data/csqa/._graph  \n",
            "  inflating: data/csqa/test_rand_split_no_answers.jsonl  \n",
            "  inflating: __MACOSX/data/csqa/._test_rand_split_no_answers.jsonl  \n",
            "   creating: data/csqa/statement/\n",
            "  inflating: __MACOSX/data/csqa/._statement  \n",
            "  inflating: data/csqa/inhouse_split_qids.txt  \n",
            "  inflating: __MACOSX/data/csqa/._inhouse_split_qids.txt  \n",
            "   creating: data/csqa/grounded/\n",
            "  inflating: __MACOSX/data/csqa/._grounded  \n",
            "  inflating: data/csqa/dev_rand_split.jsonl  \n",
            "  inflating: __MACOSX/data/csqa/._dev_rand_split.jsonl  \n",
            "  inflating: data/obqa/graph/train.graph.adj.pk-nodenum200.loaded_cache  \n",
            "  inflating: __MACOSX/data/obqa/graph/._train.graph.adj.pk-nodenum200.loaded_cache  \n",
            "  inflating: data/obqa/graph/test.graph.adj.pk-nodenum200.loaded_cache  \n",
            "  inflating: __MACOSX/data/obqa/graph/._test.graph.adj.pk-nodenum200.loaded_cache  \n",
            "  inflating: data/obqa/graph/train.graph.adj.pk  \n",
            "  inflating: __MACOSX/data/obqa/graph/._train.graph.adj.pk  \n",
            "  inflating: data/obqa/graph/dev.graph.adj.pk  \n",
            "  inflating: __MACOSX/data/obqa/graph/._dev.graph.adj.pk  \n",
            "  inflating: data/obqa/graph/dev.graph.adj.pk-nodenum200.loaded_cache  \n",
            "  inflating: __MACOSX/data/obqa/graph/._dev.graph.adj.pk-nodenum200.loaded_cache  \n",
            "  inflating: data/obqa/graph/test.graph.adj.pk  \n",
            "  inflating: __MACOSX/data/obqa/graph/._test.graph.adj.pk  \n",
            "   creating: data/obqa/fairseq/official/\n",
            "  inflating: __MACOSX/data/obqa/fairseq/._official  \n",
            "  inflating: data/obqa/statement/dev.statement.jsonl-sl100.loaded_cache  \n",
            "  inflating: __MACOSX/data/obqa/statement/._dev.statement.jsonl-sl100.loaded_cache  \n",
            "  inflating: data/obqa/statement/train.statement.jsonl  \n",
            "  inflating: __MACOSX/data/obqa/statement/._train.statement.jsonl  \n",
            "  inflating: data/obqa/statement/dev.statement.jsonl  \n",
            "  inflating: __MACOSX/data/obqa/statement/._dev.statement.jsonl  \n",
            "  inflating: data/obqa/statement/train.statement.jsonl-sl100.loaded_cache  \n",
            "  inflating: __MACOSX/data/obqa/statement/._train.statement.jsonl-sl100.loaded_cache  \n",
            "  inflating: data/obqa/statement/test.statement.jsonl-sl100.loaded_cache  \n",
            "  inflating: __MACOSX/data/obqa/statement/._test.statement.jsonl-sl100.loaded_cache  \n",
            "  inflating: data/obqa/statement/test.statement.jsonl  \n",
            "  inflating: __MACOSX/data/obqa/statement/._test.statement.jsonl  \n",
            "  inflating: data/obqa/grounded/train.grounded.jsonl  \n",
            "  inflating: __MACOSX/data/obqa/grounded/._train.grounded.jsonl  \n",
            "  inflating: data/obqa/grounded/test.grounded.jsonl  \n",
            "  inflating: __MACOSX/data/obqa/grounded/._test.grounded.jsonl  \n",
            "  inflating: data/obqa/grounded/dev.grounded.jsonl  \n",
            "  inflating: __MACOSX/data/obqa/grounded/._dev.grounded.jsonl  \n",
            "   creating: data/obqa/OpenBookQA-V1-Sep2018/Data/\n",
            "  inflating: __MACOSX/data/obqa/OpenBookQA-V1-Sep2018/._Data  \n",
            "  inflating: data/csqa/graph/train.graph.adj.pk-nodenum200.loaded_cache  \n",
            "  inflating: __MACOSX/data/csqa/graph/._train.graph.adj.pk-nodenum200.loaded_cache  \n",
            "  inflating: data/csqa/graph/test.graph.adj.pk-nodenum200.loaded_cache  \n",
            "  inflating: __MACOSX/data/csqa/graph/._test.graph.adj.pk-nodenum200.loaded_cache  \n",
            "  inflating: data/csqa/graph/train.graph.adj.pk  \n",
            "  inflating: __MACOSX/data/csqa/graph/._train.graph.adj.pk  \n",
            "  inflating: data/csqa/graph/dev.graph.adj.pk  \n",
            "  inflating: __MACOSX/data/csqa/graph/._dev.graph.adj.pk  \n",
            "  inflating: data/csqa/graph/dev.graph.adj.pk-nodenum200.loaded_cache  \n",
            "  inflating: __MACOSX/data/csqa/graph/._dev.graph.adj.pk-nodenum200.loaded_cache  \n",
            "  inflating: data/csqa/graph/test.graph.adj.pk  \n",
            "  inflating: __MACOSX/data/csqa/graph/._test.graph.adj.pk  \n",
            "  inflating: data/csqa/statement/dev.statement.jsonl-sl100.loaded_cache  \n",
            "  inflating: __MACOSX/data/csqa/statement/._dev.statement.jsonl-sl100.loaded_cache  \n",
            "  inflating: data/csqa/statement/train.statement.jsonl  \n",
            "  inflating: __MACOSX/data/csqa/statement/._train.statement.jsonl  \n",
            "  inflating: data/csqa/statement/dev.statement.jsonl  \n",
            "  inflating: __MACOSX/data/csqa/statement/._dev.statement.jsonl  \n",
            "  inflating: data/csqa/statement/train.statement.jsonl-sl100.loaded_cache  \n",
            "  inflating: __MACOSX/data/csqa/statement/._train.statement.jsonl-sl100.loaded_cache  \n",
            "  inflating: data/csqa/statement/test.statement.jsonl-sl100.loaded_cache  \n",
            "  inflating: __MACOSX/data/csqa/statement/._test.statement.jsonl-sl100.loaded_cache  \n",
            "  inflating: data/csqa/statement/test.statement.jsonl  \n",
            "  inflating: __MACOSX/data/csqa/statement/._test.statement.jsonl  \n",
            "  inflating: data/csqa/grounded/train.grounded.jsonl  \n",
            "  inflating: __MACOSX/data/csqa/grounded/._train.grounded.jsonl  \n",
            "  inflating: data/csqa/grounded/test.grounded.jsonl  \n",
            "  inflating: __MACOSX/data/csqa/grounded/._test.grounded.jsonl  \n",
            "  inflating: data/csqa/grounded/dev.grounded.jsonl  \n",
            "  inflating: __MACOSX/data/csqa/grounded/._dev.grounded.jsonl  \n",
            "  inflating: data/obqa/fairseq/official/train.jsonl  \n",
            "  inflating: __MACOSX/data/obqa/fairseq/official/._train.jsonl  \n",
            "  inflating: data/obqa/fairseq/official/test.jsonl  \n",
            "  inflating: __MACOSX/data/obqa/fairseq/official/._test.jsonl  \n",
            "  inflating: data/obqa/fairseq/official/valid.jsonl  \n",
            "  inflating: __MACOSX/data/obqa/fairseq/official/._valid.jsonl  \n",
            "   creating: data/obqa/OpenBookQA-V1-Sep2018/Data/Additional/\n",
            "  inflating: __MACOSX/data/obqa/OpenBookQA-V1-Sep2018/Data/._Additional  \n",
            "   creating: data/obqa/OpenBookQA-V1-Sep2018/Data/Main/\n",
            "  inflating: __MACOSX/data/obqa/OpenBookQA-V1-Sep2018/Data/._Main  \n",
            "  inflating: data/obqa/OpenBookQA-V1-Sep2018/Data/Additional/test_complete.jsonl  \n",
            "  inflating: __MACOSX/data/obqa/OpenBookQA-V1-Sep2018/Data/Additional/._test_complete.jsonl  \n",
            "  inflating: data/obqa/OpenBookQA-V1-Sep2018/Data/Additional/train_complete.jsonl  \n",
            "  inflating: __MACOSX/data/obqa/OpenBookQA-V1-Sep2018/Data/Additional/._train_complete.jsonl  \n",
            "  inflating: data/obqa/OpenBookQA-V1-Sep2018/Data/Additional/crowdsourced-facts.txt  \n",
            "  inflating: __MACOSX/data/obqa/OpenBookQA-V1-Sep2018/Data/Additional/._crowdsourced-facts.txt  \n",
            "  inflating: data/obqa/OpenBookQA-V1-Sep2018/Data/Additional/dev_complete.jsonl  \n",
            "  inflating: __MACOSX/data/obqa/OpenBookQA-V1-Sep2018/Data/Additional/._dev_complete.jsonl  \n",
            "  inflating: data/obqa/OpenBookQA-V1-Sep2018/Data/Main/train.jsonl  \n",
            "  inflating: __MACOSX/data/obqa/OpenBookQA-V1-Sep2018/Data/Main/._train.jsonl  \n",
            "  inflating: data/obqa/OpenBookQA-V1-Sep2018/Data/Main/test.jsonl  \n",
            "  inflating: __MACOSX/data/obqa/OpenBookQA-V1-Sep2018/Data/Main/._test.jsonl  \n",
            "  inflating: data/obqa/OpenBookQA-V1-Sep2018/Data/Main/train.tsv  \n",
            "  inflating: __MACOSX/data/obqa/OpenBookQA-V1-Sep2018/Data/Main/._train.tsv  \n",
            "  inflating: data/obqa/OpenBookQA-V1-Sep2018/Data/Main/dev.tsv  \n",
            "  inflating: __MACOSX/data/obqa/OpenBookQA-V1-Sep2018/Data/Main/._dev.tsv  \n",
            "  inflating: data/obqa/OpenBookQA-V1-Sep2018/Data/Main/dev.jsonl  \n",
            "  inflating: __MACOSX/data/obqa/OpenBookQA-V1-Sep2018/Data/Main/._dev.jsonl  \n",
            "  inflating: data/obqa/OpenBookQA-V1-Sep2018/Data/Main/openbook.txt  \n",
            "  inflating: __MACOSX/data/obqa/OpenBookQA-V1-Sep2018/Data/Main/._openbook.txt  \n",
            "  inflating: data/obqa/OpenBookQA-V1-Sep2018/Data/Main/test.tsv  \n",
            "  inflating: __MACOSX/data/obqa/OpenBookQA-V1-Sep2018/Data/Main/._test.tsv  \n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npNatDvdtmpf",
        "outputId": "5213c64a-e9fe-4515-abf2-51c1197f8caa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/graphprefix/data\n"
          ]
        }
      ],
      "source": [
        "cd data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InMNocpOtLsL",
        "outputId": "2f5b2d71-414c-427d-a2d7-b012bc6d7a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17Hoi428V4gDzHanfsi-9D18EzTGD1Xay\n",
            "To: /content/graphprefix/data/ddb.zip\n",
            "100% 34.7M/34.7M [00:01<00:00, 32.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 17Hoi428V4gDzHanfsi-9D18EzTGD1Xay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6ajCKjmtMYO",
        "outputId": "9990e371-8d91-4d8c-b15a-20a503b37e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oW3jcn4qiUWaFqPe7jglvLimrW8EfS8X\n",
            "To: /content/graphprefix/data/medqa_usmle.zip\n",
            "100% 67.9M/67.9M [00:01<00:00, 67.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1oW3jcn4qiUWaFqPe7jglvLimrW8EfS8X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CawydmUteEd",
        "outputId": "2cba4f6d-6987-4bb3-cb2a-33e45c60626f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  ddb.zip\n",
            "   creating: ddb/\n",
            "  inflating: __MACOSX/._ddb          \n",
            "  inflating: ddb/ent_emb.npy         \n",
            "  inflating: __MACOSX/ddb/._ent_emb.npy  \n",
            "  inflating: ddb/ddb.graph           \n",
            "  inflating: __MACOSX/ddb/._ddb.graph  \n",
            "  inflating: ddb/ptrs.txt            \n",
            "  inflating: __MACOSX/ddb/._ptrs.txt  \n",
            "  inflating: ddb/ddb_names.json      \n",
            "  inflating: __MACOSX/ddb/._ddb_names.json  \n",
            "  inflating: ddb/ddb_relas.json      \n",
            "  inflating: __MACOSX/ddb/._ddb_relas.json  \n",
            "  inflating: ddb/ddb_to_umls_cui.txt  \n",
            "  inflating: __MACOSX/ddb/._ddb_to_umls_cui.txt  \n",
            "  inflating: ddb/vocab.txt           \n",
            "  inflating: __MACOSX/ddb/._vocab.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip ddb.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZeRqPaSt3v_",
        "outputId": "d0c0bd0a-a81d-4053-beb9-7efb15068685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  medqa_usmle.zip\n",
            "   creating: medqa_usmle/\n",
            "  inflating: __MACOSX/._medqa_usmle  \n",
            "   creating: medqa_usmle/graph/\n",
            "  inflating: __MACOSX/medqa_usmle/._graph  \n",
            "   creating: medqa_usmle/statement/\n",
            "  inflating: __MACOSX/medqa_usmle/._statement  \n",
            "   creating: medqa_usmle/grounded/\n",
            "  inflating: __MACOSX/medqa_usmle/._grounded  \n",
            "  inflating: medqa_usmle/graph/train.graph.adj.pk  \n",
            "  inflating: __MACOSX/medqa_usmle/graph/._train.graph.adj.pk  \n",
            "  inflating: medqa_usmle/graph/train.graph.adj.pk.loaded_cache  \n",
            "  inflating: __MACOSX/medqa_usmle/graph/._train.graph.adj.pk.loaded_cache  \n",
            "  inflating: medqa_usmle/graph/dev.graph.adj.pk  \n",
            "  inflating: __MACOSX/medqa_usmle/graph/._dev.graph.adj.pk  \n",
            "  inflating: medqa_usmle/graph/dev.graph.adj.pk.loaded_cache  \n",
            "  inflating: __MACOSX/medqa_usmle/graph/._dev.graph.adj.pk.loaded_cache  \n",
            "  inflating: medqa_usmle/graph/test.graph.adj.pk.loaded_cache  \n",
            "  inflating: __MACOSX/medqa_usmle/graph/._test.graph.adj.pk.loaded_cache  \n",
            "  inflating: medqa_usmle/graph/test.graph.adj.pk  \n",
            "  inflating: __MACOSX/medqa_usmle/graph/._test.graph.adj.pk  \n",
            "  inflating: medqa_usmle/statement/train.statement.umls_linked.jsonl  \n",
            "  inflating: __MACOSX/medqa_usmle/statement/._train.statement.umls_linked.jsonl  \n",
            "  inflating: medqa_usmle/statement/train.statement.jsonl  \n",
            "  inflating: __MACOSX/medqa_usmle/statement/._train.statement.jsonl  \n",
            "  inflating: medqa_usmle/statement/dev.statement.jsonl  \n",
            "  inflating: __MACOSX/medqa_usmle/statement/._dev.statement.jsonl  \n",
            "  inflating: medqa_usmle/statement/dev.statement.umls_linked.jsonl  \n",
            "  inflating: __MACOSX/medqa_usmle/statement/._dev.statement.umls_linked.jsonl  \n",
            "  inflating: medqa_usmle/statement/test.statement.umls_linked.jsonl  \n",
            "  inflating: __MACOSX/medqa_usmle/statement/._test.statement.umls_linked.jsonl  \n",
            "  inflating: medqa_usmle/statement/test.statement.jsonl  \n",
            "  inflating: __MACOSX/medqa_usmle/statement/._test.statement.jsonl  \n",
            "  inflating: medqa_usmle/grounded/train.grounded.jsonl  \n",
            "  inflating: __MACOSX/medqa_usmle/grounded/._train.grounded.jsonl  \n",
            "  inflating: medqa_usmle/grounded/test.grounded.jsonl  \n",
            "  inflating: __MACOSX/medqa_usmle/grounded/._test.grounded.jsonl  \n",
            "  inflating: medqa_usmle/grounded/dev.grounded.jsonl  \n",
            "  inflating: __MACOSX/medqa_usmle/grounded/._dev.grounded.jsonl  \n"
          ]
        }
      ],
      "source": [
        "!unzip medqa_usmle.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zZqbEgNt_n1"
      },
      "outputs": [],
      "source": [
        "!rm -r medqa_usmle.zip\n",
        "!rm -r ddb.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKOLKApmum90",
        "outputId": "5b3ed467-48a4-4bf3-efc6-af7cfc76f214"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.5.18.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 59.7 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 65.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.17-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 86.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 60.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=e935398dfb4fe16869a55ea33936d72c90e787ee9b89b1099f2afae9a598351a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.17\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardx\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.15.0)\n",
            "Installing collected packages: tensorboardx\n",
            "Successfully installed tensorboardx-2.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.3.7)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.35.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.21.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.46.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (4.2.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy tqdm\n",
        "!pip install torch torchvision\n",
        "!pip install transformers nltk spacy\n",
        "!pip install wandb\n",
        "!pip install tensorboardx\n",
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeUBAJi6AsJT",
        "outputId": "53e35837-e8c4-4106-e821-c4a86b6fb276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.11.0+cu113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 4.5 MB/s \n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 1.1 MB/s \n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (750 kB)\n",
            "\u001b[K     |████████████████████████████████| 750 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.5.18.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=e54e1b50f05f45b3c08c35514fa7def9a940cf7f74f265bc691c2a09825e6c8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-spline-conv, torch-sparse, torch-scatter, torch-geometric, torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0 torch-geometric-2.0.4 torch-scatter-2.0.9 torch-sparse-0.6.13 torch-spline-conv-1.2.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-{torch.__version__}.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH9Gxf3S_8XT",
        "outputId": "289ddd94-471a-402e-9712-1330c5946769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.11\n",
            "  Downloading transformers-4.11.0-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.11) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.11) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.11) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 66.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers==4.11) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.11) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.11) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11) (2022.5.18.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=1a99bb1b77852b5a3c5d867353ab1474f4947f7c70700c28f276815a7b4dd201\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.12.1\n",
            "    Uninstalling tokenizers-0.12.1:\n",
            "      Successfully uninstalled tokenizers-0.12.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.19.2\n",
            "    Uninstalling transformers-4.19.2:\n",
            "      Successfully uninstalled transformers-4.19.2\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbbDmBn-gzvN",
        "outputId": "86abdb86-798b-4631-e829-943f0ddd2640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/graphprefix\n"
          ]
        }
      ],
      "source": [
        "cd /content/graphprefix/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLu6miUUfpR-",
        "outputId": "e12b90fd-eb03-4ac0-d313-ba37141fa085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue May 31 19:01:39 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaDkE-JnuXk3",
        "outputId": "640fbb76-cd31-465c-ce57-76e1d27d388f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** hyperparameters *****\n",
            "dataset: csqa\n",
            "enc_name: roberta-base\n",
            "batch_size: 128 mini_batch_size: 8\n",
            "learning_rate: elr 1e-3 dlr 1e-3\n",
            "gnn: dim 256 layer 3\n",
            "ie_dim: 400, info_exchange: true\n",
            "******************************\n",
            "8bb1217fe987\n",
            "pid: 432\n",
            "screen: \n",
            "\n",
            "gpu: \n",
            "\n",
            "torch version: 1.11.0+cu113\n",
            "torch cuda version: 11.3\n",
            "cuda is available: False\n",
            "cuda device count: 0\n",
            "cudnn version: 8200\n",
            "wandb id:  nbpbb2xh\n",
            "args: Namespace(att_head_num=2, batch_size=128, cache_dir=None, config_name=None, cuda=True, cxt_node_connects_all=False, data_dir='data/', dataset='csqa', debug=False, decoder_lr=0.001, dev_adj='data//csqa/graph/dev.graph.adj.pk', dev_statements='data//csqa/statement/dev.statement.jsonl', dropoutf=0.2, dropoutg=0.2, dropouti=0.2, encoder='roberta-base', encoder_layer=-1, encoder_lr=0.001, ent_emb=['tzw'], ent_emb_paths=['data/cpnet/tzw.ent.npy'], eval_batch_size=2, fc_dim=200, fc_layer_num=0, freeze_ent_emb=True, gnn_dim=256, hf_version='4.11.0', hidden_dropout_prob=0.1, ie_dim=400, ie_layer_num=1, info_exchange=True, inhouse=False, inhouse_train_qids='data/csqa/inhouse_split_qids.txt', init_range=0.02, k=3, load_model_path=None, log_interval=10, loss='cross_entropy', lr_schedule='fixed', max_epochs_before_stop=10, max_grad_norm=1.0, max_node_num=200, max_seq_len=100, mini_batch_size=8, mode='train', model_name_or_path='roberta-base', model_revision='main', n_epochs=300, n_etype=38, n_ntype=4, n_train=-1, optim='radam', pre_seq_len=32, prefix=False, prefix_cross_attention=True, prefix_hidden_size=256, prefix_projection=False, prompt=False, random_ent_emb=False, refreeze_epoch=10000, resume_checkpoint='None', resume_id='None', run_name='prefixfusion__ds_csqa__enc_roberta-base__k3__sd5__iedim400__20220531_190455', save_dir='runs/csqa/prefixfusion__ds_csqa__enc_roberta-base__k3__sd5__iedim400__20220531_190455', save_model=True, seed=5, sep_ie_layers=False, subsample=1.0, test_adj='data//csqa/graph/test.graph.adj.pk', test_statements='data//csqa/statement/test.statement.jsonl', tokenizer_name=None, train_adj='data//csqa/graph/train.graph.adj.pk', train_statements='data//csqa/statement/train.statement.jsonl', unfreeze_epoch=4, use_auth_token=False, use_fast_tokenizer=True, use_wandb=False, wandb_id='nbpbb2xh', warmup_steps=150, weight_decay=0.01)\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n",
            "train_statement_path data//csqa/statement/train.statement.jsonl\n",
            "num_choice 5\n",
            "Loading sparse adj data...\n",
            "| ori_adj_len: mu 121.04 sigma 93.93 | adj_len: 107.56 | prune_rate： 0.17 | qc_num: 7.41 | ac_num: 2.10 |\n",
            "Finish loading training data.\n",
            "Loading sparse adj data...\n",
            "| ori_adj_len: mu 118.21 sigma 90.69 | adj_len: 106.36 | prune_rate： 0.15 | qc_num: 7.21 | ac_num: 2.08 |\n",
            "Finish loading dev data.\n",
            "Loading sparse adj data...\n",
            "| ori_adj_len: mu 118.27 sigma 93.53 | adj_len: 105.30 | prune_rate： 0.16 | qc_num: 7.33 | ac_num: 2.06 |\n",
            "Finish loading test data.\n",
            "tcmalloc: large alloc 3273826304 bytes == 0x1136cc000 @  0x7fee9a43a1e7 0x7fee9420d0ce 0x7fee94264e57 0x7fee94265a6f 0x7fee9430bc5d 0x593835 0x548c51 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549e0e 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206\n",
            "tcmalloc: large alloc 3273826304 bytes == 0x1d68f6000 @  0x7fee9a43a1e7 0x7fee9420d0ce 0x7fee94263cf5 0x7fee9430c86d 0x7fee9430d17f 0x7fee9430d2d0 0x4bc4ab 0x7fee9424e944 0x59371f 0x515244 0x549576 0x593fce 0x548ae9 0x51566f 0x593dd7 0x5118f8 0x549e0e 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7fee9a037c87 0x5b621a\n",
            "tcmalloc: large alloc 3273826304 bytes == 0x1136cc000 @  0x7fee9a41cb6b 0x7fee9a43c379 0x7fedd15ed50e 0x7fedd15df7c2 0x7fee0b5501a7 0x7fee0b550449 0x7fee0b5504a6 0x7fee0ba249f8 0x7fee0c111c0f 0x7fee0bed3656 0x7fee0c0f7466 0x7fee0bf11367 0x7fee0ba21dad 0x7fee0c2138ea 0x7fee0bcda5f9 0x7fee0c0f8a47 0x7fee0bcda5f9 0x7fee0cd2c4c6 0x7fee0cd2ca0d 0x7fee0bd47f4a 0x7fee0ba1d3c9 0x7fee0c2aa2c2 0x7fee0be3a813 0x7fee86ede866 0x7fee86ee521e 0x7fee86bee9bf 0x593835 0x548c51 0x5127f1 0x593dd7 0x5118f8\n",
            "| num_concepts: 799273 |\n",
            "tcmalloc: large alloc 3273834496 bytes == 0x29a3fa000 @  0x7fee9a41cb6b 0x7fee9a43c379 0x7fedd15ed50e 0x7fedd15df7c2 0x7fee0b55110f 0x7fee0b551a51 0x7fee0b551aa4 0x7fee0ba249ae 0x7fee0c111aea 0x7fee0bed338c 0x7fee0c0f862f 0x7fee0bf0e1e0 0x7fee86ad2754 0x517553 0x549e0e 0x4bcb19 0x532b86 0x594a96 0x548cc1 0x51566f 0x549e0e 0x4bca8a 0x532b86 0x594a96 0x548cc1 0x5127f1 0x549e0e 0x4bcb19 0x532b86 0x53786a 0x595ef6\n",
            "total param is 821575426\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaPrefixCrossAttentionForMultipleChoice: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaPrefixCrossAttentionForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaPrefixCrossAttentionForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaPrefixCrossAttentionForMultipleChoice were not initialized from the model checkpoint at roberta-base and are newly initialized: ['emb_node_type.bias', 'ie_layer.layers.1-Linear.bias', 'gnn_layers.0.mlp.0.bias', 'edge_encoder.0.bias', 'gnn_layers.0.linear_key.bias', 'ie_layer.layers.1-Linear.weight', 'ie_layer.layers.0-Linear.bias', 'gnn_layers.2.linear_key.weight', 'gnn_layers.2.edge_encoder.0.bias', 'gnn_layers.2.mlp.0.bias', 'ie_layer.layers.0-Linear.weight', 'gnn_layers.0.edge_encoder.1.weight', 'gnn_layers.0.edge_encoder.3.weight', 'gnn_layers.1.edge_encoder.0.weight', 'gnn_layers.2.linear_query.weight', 'gnn_layers.1.mlp.1.running_var', 'gnn_layers.2.edge_encoder.1.weight', 'gnn_layers.0.edge_encoder.1.running_mean', 'gnn_layers.2.edge_encoder.3.weight', 'gnn_layers.1.mlp.0.bias', 'emb_node_type.weight', 'gnn_layers.2.mlp.1.bias', 'gnn_layers.2.edge_encoder.1.bias', 'gnn_layers.0.mlp.3.weight', 'gnn_layers.0.mlp.1.bias', 'concept_emb.cpt_transform.bias', 'edge_encoder.1.bias', 'gnn_layers.0.mlp.3.bias', 'gnn_layers.1.linear_msg.bias', 'gnn_layers.1.mlp.1.bias', 'gnn_layers.1.edge_encoder.1.weight', 'gnn_layers.1.edge_encoder.1.running_mean', 'gnn_layers.0.edge_encoder.0.bias', 'classifier.weight', 'gnn_layers.0.edge_encoder.1.num_batches_tracked', 'gnn_layers.0.edge_encoder.1.bias', 'edge_encoder.1.weight', 'gnn_layers.0.linear_query.bias', 'gnn_layers.0.mlp.1.running_var', 'concept_emb.emb.weight', 'gnn_layers.1.mlp.3.weight', 'classifier2.bias', 'edge_encoder.1.num_batches_tracked', 'gnn_layers.1.edge_encoder.0.bias', 'classifier.bias', 'edge_encoder.0.weight', 'gnn_layers.1.mlp.1.weight', 'gnn_layers.1.mlp.1.num_batches_tracked', 'edge_encoder.1.running_mean', 'gnn_layers.2.edge_encoder.1.num_batches_tracked', 'gnn_layers.1.edge_encoder.1.running_var', 'concept_emb.cpt_transform.weight', 'gnn_layers.0.linear_query.weight', 'gnn_layers.2.mlp.1.weight', 'edge_encoder.3.bias', 'gnn_layers.1.edge_encoder.1.num_batches_tracked', 'gnn_layers.1.mlp.0.weight', 'gnn_layers.0.linear_msg.bias', 'gnn_layers.2.linear_msg.bias', 'gnn_layers.2.mlp.0.weight', 'edge_encoder.1.running_var', 'gnn_layers.2.mlp.3.weight', 'gnn_layers.1.linear_query.bias', 'emb_score.weight', 'gnn_layers.1.edge_encoder.3.weight', 'gnn_layers.1.mlp.3.bias', 'gnn_layers.1.linear_msg.weight', 'gnn_layers.2.linear_query.bias', 'gnn_layers.0.mlp.1.weight', 'gnn_layers.2.edge_encoder.1.running_mean', 'gnn_layers.1.edge_encoder.1.bias', 'gnn_layers.2.linear_key.bias', 'gnn_layers.2.mlp.1.running_var', 'gnn_layers.0.mlp.1.running_mean', 'gnn_layers.2.linear_msg.weight', 'gnn_layers.2.edge_encoder.1.running_var', 'gnn_layers.2.mlp.1.running_mean', 'gnn_layers.0.mlp.0.weight', 'gnn_layers.1.linear_query.weight', 'emb_score.bias', 'gnn_layers.1.linear_key.weight', 'gnn_layers.1.edge_encoder.3.bias', 'edge_encoder.3.weight', 'gnn_layers.0.linear_key.weight', 'gnn_layers.0.edge_encoder.0.weight', 'gnn_layers.0.mlp.1.num_batches_tracked', 'gnn_layers.0.edge_encoder.3.bias', 'gnn_layers.1.linear_key.bias', 'gnn_layers.2.edge_encoder.0.weight', 'gnn_layers.2.mlp.3.bias', 'classifier2.weight', 'gnn_layers.1.mlp.1.running_mean', 'gnn_layers.0.linear_msg.weight', 'gnn_layers.2.mlp.1.num_batches_tracked', 'gnn_layers.2.edge_encoder.3.bias', 'prefix_encoder.embedding.weight', 'gnn_layers.0.edge_encoder.1.running_var']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "len(dataset.tokenizer): 50265\n",
            "Non-loaded parameters:\n",
            "\troberta.embeddings.word_embeddings.weight    \tfixed\ttorch.Size([50265, 768])\tdevice:cpu\n",
            "\troberta.embeddings.position_embeddings.weight\tfixed\ttorch.Size([514, 768])\tdevice:cpu\n",
            "\troberta.embeddings.token_type_embeddings.weight\tfixed\ttorch.Size([1, 768])\tdevice:cpu\n",
            "\troberta.embeddings.LayerNorm.weight          \tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.embeddings.LayerNorm.bias            \tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.attention.self.query.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.attention.self.query.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.attention.self.key.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.attention.self.key.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.attention.self.value.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.attention.self.value.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.attention.output.dense.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.attention.output.dense.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.attention.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.attention.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.intermediate.dense.weight\tfixed\ttorch.Size([3072, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.intermediate.dense.bias\tfixed\ttorch.Size([3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.output.dense.weight  \tfixed\ttorch.Size([768, 3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.output.dense.bias    \tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.0.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.attention.self.query.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.attention.self.query.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.attention.self.key.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.attention.self.key.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.attention.self.value.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.attention.self.value.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.attention.output.dense.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.attention.output.dense.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.attention.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.attention.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.intermediate.dense.weight\tfixed\ttorch.Size([3072, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.intermediate.dense.bias\tfixed\ttorch.Size([3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.output.dense.weight  \tfixed\ttorch.Size([768, 3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.output.dense.bias    \tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.1.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.attention.self.query.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.attention.self.query.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.attention.self.key.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.attention.self.key.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.attention.self.value.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.attention.self.value.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.attention.output.dense.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.attention.output.dense.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.attention.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.attention.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.intermediate.dense.weight\tfixed\ttorch.Size([3072, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.intermediate.dense.bias\tfixed\ttorch.Size([3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.output.dense.weight  \tfixed\ttorch.Size([768, 3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.output.dense.bias    \tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.2.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.attention.self.query.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.attention.self.query.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.attention.self.key.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.attention.self.key.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.attention.self.value.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.attention.self.value.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.attention.output.dense.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.attention.output.dense.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.attention.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.attention.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.intermediate.dense.weight\tfixed\ttorch.Size([3072, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.intermediate.dense.bias\tfixed\ttorch.Size([3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.output.dense.weight  \tfixed\ttorch.Size([768, 3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.output.dense.bias    \tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.3.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.attention.self.query.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.attention.self.query.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.attention.self.key.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.attention.self.key.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.attention.self.value.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.attention.self.value.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.attention.output.dense.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.attention.output.dense.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.attention.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.attention.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.intermediate.dense.weight\tfixed\ttorch.Size([3072, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.intermediate.dense.bias\tfixed\ttorch.Size([3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.output.dense.weight  \tfixed\ttorch.Size([768, 3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.output.dense.bias    \tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.4.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.attention.self.query.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.attention.self.query.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.attention.self.key.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.attention.self.key.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.attention.self.value.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.attention.self.value.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.attention.output.dense.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.attention.output.dense.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.attention.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.attention.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.intermediate.dense.weight\tfixed\ttorch.Size([3072, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.intermediate.dense.bias\tfixed\ttorch.Size([3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.output.dense.weight  \tfixed\ttorch.Size([768, 3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.output.dense.bias    \tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.5.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.attention.self.query.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.attention.self.query.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.attention.self.key.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.attention.self.key.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.attention.self.value.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.attention.self.value.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.attention.output.dense.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.attention.output.dense.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.attention.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.attention.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.intermediate.dense.weight\tfixed\ttorch.Size([3072, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.intermediate.dense.bias\tfixed\ttorch.Size([3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.output.dense.weight  \tfixed\ttorch.Size([768, 3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.output.dense.bias    \tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.6.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.attention.self.query.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.attention.self.query.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.attention.self.key.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.attention.self.key.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.attention.self.value.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.attention.self.value.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.attention.output.dense.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.attention.output.dense.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.attention.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.attention.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.intermediate.dense.weight\tfixed\ttorch.Size([3072, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.intermediate.dense.bias\tfixed\ttorch.Size([3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.output.dense.weight  \tfixed\ttorch.Size([768, 3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.output.dense.bias    \tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.7.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.attention.self.query.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.attention.self.query.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.attention.self.key.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.attention.self.key.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.attention.self.value.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.attention.self.value.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.attention.output.dense.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.attention.output.dense.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.attention.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.attention.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.intermediate.dense.weight\tfixed\ttorch.Size([3072, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.intermediate.dense.bias\tfixed\ttorch.Size([3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.output.dense.weight  \tfixed\ttorch.Size([768, 3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.output.dense.bias    \tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.8.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.attention.self.query.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.attention.self.query.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.attention.self.key.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.attention.self.key.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.attention.self.value.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.attention.self.value.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.attention.output.dense.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.attention.output.dense.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.attention.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.attention.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.intermediate.dense.weight\tfixed\ttorch.Size([3072, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.intermediate.dense.bias\tfixed\ttorch.Size([3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.output.dense.weight  \tfixed\ttorch.Size([768, 3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.output.dense.bias    \tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.9.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.attention.self.query.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.attention.self.query.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.attention.self.key.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.attention.self.key.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.attention.self.value.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.attention.self.value.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.attention.output.dense.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.attention.output.dense.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.attention.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.attention.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.intermediate.dense.weight\tfixed\ttorch.Size([3072, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.intermediate.dense.bias\tfixed\ttorch.Size([3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.output.dense.weight \tfixed\ttorch.Size([768, 3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.output.dense.bias   \tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.10.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.attention.self.query.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.attention.self.query.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.attention.self.key.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.attention.self.key.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.attention.self.value.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.attention.self.value.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.attention.output.dense.weight\tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.attention.output.dense.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.attention.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.attention.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.intermediate.dense.weight\tfixed\ttorch.Size([3072, 768])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.intermediate.dense.bias\tfixed\ttorch.Size([3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.output.dense.weight \tfixed\ttorch.Size([768, 3072])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.output.dense.bias   \tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.output.LayerNorm.weight\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.encoder.layer.11.output.LayerNorm.bias\tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\troberta.pooler.dense.weight                  \tfixed\ttorch.Size([768, 768])\tdevice:cpu\n",
            "\troberta.pooler.dense.bias                    \tfixed\ttorch.Size([768])\tdevice:cpu\n",
            "\tclassifier.weight                            \ttrainable\ttorch.Size([1, 768])\tdevice:cpu\n",
            "\tclassifier.bias                              \ttrainable\ttorch.Size([1])\tdevice:cpu\n",
            "\tclassifier2.weight                           \ttrainable\ttorch.Size([1, 256])\tdevice:cpu\n",
            "\tclassifier2.bias                             \ttrainable\ttorch.Size([1])\tdevice:cpu\n",
            "\tedge_encoder.0.weight                        \ttrainable\ttorch.Size([256, 47])\tdevice:cpu\n",
            "\tedge_encoder.0.bias                          \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tedge_encoder.1.weight                        \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tedge_encoder.1.bias                          \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tedge_encoder.3.weight                        \ttrainable\ttorch.Size([256, 256])\tdevice:cpu\n",
            "\tedge_encoder.3.bias                          \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.0.linear_key.weight               \ttrainable\ttorch.Size([256, 768])\tdevice:cpu\n",
            "\tgnn_layers.0.linear_key.bias                 \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.0.linear_msg.weight               \ttrainable\ttorch.Size([256, 768])\tdevice:cpu\n",
            "\tgnn_layers.0.linear_msg.bias                 \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.0.linear_query.weight             \ttrainable\ttorch.Size([256, 512])\tdevice:cpu\n",
            "\tgnn_layers.0.linear_query.bias               \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.0.mlp.0.weight                    \ttrainable\ttorch.Size([256, 256])\tdevice:cpu\n",
            "\tgnn_layers.0.mlp.0.bias                      \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.0.mlp.1.weight                    \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.0.mlp.1.bias                      \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.0.mlp.3.weight                    \ttrainable\ttorch.Size([256, 256])\tdevice:cpu\n",
            "\tgnn_layers.0.mlp.3.bias                      \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.1.linear_key.weight               \ttrainable\ttorch.Size([256, 768])\tdevice:cpu\n",
            "\tgnn_layers.1.linear_key.bias                 \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.1.linear_msg.weight               \ttrainable\ttorch.Size([256, 768])\tdevice:cpu\n",
            "\tgnn_layers.1.linear_msg.bias                 \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.1.linear_query.weight             \ttrainable\ttorch.Size([256, 512])\tdevice:cpu\n",
            "\tgnn_layers.1.linear_query.bias               \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.1.mlp.0.weight                    \ttrainable\ttorch.Size([256, 256])\tdevice:cpu\n",
            "\tgnn_layers.1.mlp.0.bias                      \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.1.mlp.1.weight                    \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.1.mlp.1.bias                      \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.1.mlp.3.weight                    \ttrainable\ttorch.Size([256, 256])\tdevice:cpu\n",
            "\tgnn_layers.1.mlp.3.bias                      \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.2.linear_key.weight               \ttrainable\ttorch.Size([256, 768])\tdevice:cpu\n",
            "\tgnn_layers.2.linear_key.bias                 \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.2.linear_msg.weight               \ttrainable\ttorch.Size([256, 768])\tdevice:cpu\n",
            "\tgnn_layers.2.linear_msg.bias                 \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.2.linear_query.weight             \ttrainable\ttorch.Size([256, 512])\tdevice:cpu\n",
            "\tgnn_layers.2.linear_query.bias               \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.2.mlp.0.weight                    \ttrainable\ttorch.Size([256, 256])\tdevice:cpu\n",
            "\tgnn_layers.2.mlp.0.bias                      \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.2.mlp.1.weight                    \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.2.mlp.1.bias                      \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tgnn_layers.2.mlp.3.weight                    \ttrainable\ttorch.Size([256, 256])\tdevice:cpu\n",
            "\tgnn_layers.2.mlp.3.bias                      \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tconcept_emb.emb.weight                       \tfixed\ttorch.Size([799275, 1024])\tdevice:cpu\n",
            "\tconcept_emb.cpt_transform.weight             \ttrainable\ttorch.Size([256, 1024])\tdevice:cpu\n",
            "\tconcept_emb.cpt_transform.bias               \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\temb_node_type.weight                         \ttrainable\ttorch.Size([128, 4])\tdevice:cpu\n",
            "\temb_node_type.bias                           \ttrainable\ttorch.Size([128])\tdevice:cpu\n",
            "\tie_layer.layers.0-Linear.weight              \ttrainable\ttorch.Size([256, 256])\tdevice:cpu\n",
            "\tie_layer.layers.0-Linear.bias                \ttrainable\ttorch.Size([256])\tdevice:cpu\n",
            "\tie_layer.layers.1-Linear.weight              \ttrainable\ttorch.Size([512, 256])\tdevice:cpu\n",
            "\tie_layer.layers.1-Linear.bias                \ttrainable\ttorch.Size([512])\tdevice:cpu\n",
            "\temb_score.weight                             \ttrainable\ttorch.Size([128, 128])\tdevice:cpu\n",
            "\temb_score.bias                               \ttrainable\ttorch.Size([128])\tdevice:cpu\n",
            "\tprefix_encoder.embedding.weight              \ttrainable\ttorch.Size([32, 18432])\tdevice:cpu\n",
            "num_trainable_params: 3117826\n",
            "num_fixed_params: 943103232\n",
            "num_loaded_params: 0\n",
            "num_total_params: 946221058\n",
            "\n",
            "-----------------------------------------------------------------------\n",
            "Epoch:   0% 0/300 [00:00<?, ?it/s]\n",
            "Batch:   0% 0/77 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/content/graphprefix/utils/optimization_utils.py:61: UserWarning: This overload of addcmul_ is deprecated:\n",
            "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
            "Consider using one of the following signatures instead:\n",
            "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1055.)\n",
            "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
            "\n",
            "Batch:   1% 1/77 [04:49<6:07:01, 289.76s/it]\u001b[A\n",
            "Batch:   3% 2/77 [09:24<5:51:21, 281.09s/it]\u001b[A\n",
            "Batch:   4% 3/77 [14:11<5:49:45, 283.59s/it]\u001b[A\n",
            "Batch:   5% 4/77 [18:57<5:46:22, 284.70s/it]\u001b[A\n",
            "Batch:   6% 5/77 [23:39<5:40:10, 283.48s/it]\u001b[A\n",
            "Batch:   8% 6/77 [28:28<5:38:01, 285.66s/it]\u001b[A\n",
            "Batch:   9% 7/77 [33:17<5:34:30, 286.72s/it]\u001b[A\n",
            "Batch:  10% 8/77 [38:08<5:31:01, 287.84s/it]\u001b[A\n",
            "Batch:  12% 9/77 [42:56<5:26:29, 288.08s/it]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "| step     9 |  lr: 0.0010000 | total loss  1.6104 | ms/batch 286649.27 |\n",
            "\n",
            "Batch:  13% 10/77 [47:46<5:22:17, 288.62s/it]\u001b[A\n",
            "Batch:  14% 11/77 [52:33<5:17:02, 288.23s/it]\u001b[A\n",
            "Batch:  16% 12/77 [57:20<5:11:50, 287.85s/it]\u001b[A\n",
            "Batch:  17% 13/77 [1:02:07<5:06:44, 287.57s/it]\u001b[A\n",
            "Batch:  18% 14/77 [1:07:00<5:03:39, 289.19s/it]\u001b[A\n",
            "Batch:  19% 15/77 [1:11:53<5:00:05, 290.41s/it]\u001b[A\n",
            "Batch:  21% 16/77 [1:16:38<4:53:28, 288.66s/it]\u001b[A\n",
            "Batch:  22% 17/77 [1:21:27<4:48:39, 288.66s/it]\u001b[A\n",
            "Batch:  23% 18/77 [1:26:11<4:42:33, 287.35s/it]\u001b[A\n",
            "Batch:  25% 19/77 [1:30:54<4:36:36, 286.15s/it]\u001b[A| step    19 |  lr: 0.0010000 | total loss  1.6098 | ms/batch 287863.41 |\n",
            "\n",
            "Batch:  26% 20/77 [1:35:45<4:33:02, 287.41s/it]\u001b[A\n",
            "Batch:  27% 21/77 [1:40:29<4:27:20, 286.44s/it]\u001b[A\n",
            "Batch:  29% 22/77 [1:45:11<4:21:20, 285.10s/it]\u001b[A\n",
            "Batch:  30% 23/77 [1:50:06<4:19:19, 288.13s/it]\u001b[A\n",
            "Batch:  31% 24/77 [1:54:50<4:13:24, 286.89s/it]\u001b[A\n",
            "Batch:  32% 25/77 [1:59:38<4:08:52, 287.17s/it]\u001b[A\n",
            "Batch:  34% 26/77 [2:04:20<4:02:45, 285.59s/it]\u001b[A\n",
            "Batch:  35% 27/77 [2:09:11<3:59:30, 287.42s/it]\u001b[A\n",
            "Batch:  36% 28/77 [2:13:54<3:53:31, 285.95s/it]\u001b[A\n",
            "Batch:  38% 29/77 [2:18:48<3:50:42, 288.38s/it]\u001b[A| step    29 |  lr: 0.0010000 | total loss  1.6113 | ms/batch 287115.70 |\n",
            "\n",
            "Batch:  39% 30/77 [2:23:36<3:45:46, 288.23s/it]\u001b[A\n",
            "Batch:  40% 31/77 [2:28:30<3:42:22, 290.05s/it]\u001b[A\n",
            "Batch:  42% 32/77 [2:33:17<3:36:54, 289.22s/it]\u001b[A\n",
            "Batch:  43% 33/77 [2:38:11<3:33:04, 290.55s/it]\u001b[A\n",
            "Batch:  44% 34/77 [2:43:03<3:28:27, 290.88s/it]\u001b[A\n",
            "Batch:  45% 35/77 [2:47:58<3:24:32, 292.19s/it]\u001b[A\n",
            "Batch:  47% 36/77 [2:52:51<3:19:51, 292.47s/it]\u001b[A\n",
            "Batch:  48% 37/77 [2:57:44<3:15:00, 292.51s/it]\u001b[A"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES='' ./run_prefix_fusion.sh csqa --data_dir data/ --inhouse False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBD5ieQPrtLo",
        "outputId": "cd43c232-6ef8-4b36-e60b-2cc92769bf9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 52.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME_TO_CLASS = {model_name: model_class for model_class, model_name_list in MODEL_CLASS_TO_NAME.items() for model_name in model_name_list}\n"
      ],
      "metadata": {
        "id": "n9lQLgSMsDmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(MODEL_NAME_TO_CLASS)"
      ],
      "metadata": {
        "id": "ugouRBq2sEkg",
        "outputId": "1fe0d1fe-5812-4944-8ef8-92ed1b2a5462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'openai-gpt': 'gpt', 'bert-base-uncased': 'bert', 'bert-large-uncased': 'bert', 'bert-base-cased': 'bert', 'bert-large-cased': 'bert', 'bert-base-multilingual-uncased': 'bert', 'bert-base-multilingual-cased': 'bert', 'bert-base-chinese': 'bert', 'bert-base-german-cased': 'bert', 'bert-large-uncased-whole-word-masking': 'bert', 'bert-large-cased-whole-word-masking': 'bert', 'bert-large-uncased-whole-word-masking-finetuned-squad': 'bert', 'bert-large-cased-whole-word-masking-finetuned-squad': 'bert', 'bert-base-cased-finetuned-mrpc': 'bert', 'bert-base-german-dbmdz-cased': 'bert', 'bert-base-german-dbmdz-uncased': 'bert', 'cl-tohoku/bert-base-japanese': 'bert', 'cl-tohoku/bert-base-japanese-whole-word-masking': 'bert', 'cl-tohoku/bert-base-japanese-char': 'bert', 'cl-tohoku/bert-base-japanese-char-whole-word-masking': 'bert', 'TurkuNLP/bert-base-finnish-cased-v1': 'bert', 'TurkuNLP/bert-base-finnish-uncased-v1': 'bert', 'wietsedv/bert-base-dutch-cased': 'bert', 'xlnet-base-cased': 'xlnet', 'xlnet-large-cased': 'xlnet', 'roberta-base': 'roberta', 'roberta-large': 'roberta', 'roberta-large-mnli': 'roberta', 'distilroberta-base': 'roberta', 'roberta-base-openai-detector': 'roberta', 'roberta-large-openai-detector': 'roberta', 'lstm': 'lstm'}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Graph Prefix.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPiwj4Y4h/9/yf/eMgvwpeW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}